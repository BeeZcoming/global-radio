import json
import urllib.request
import urllib.error
import os
import time
from datetime import datetime
import ssl
import math

def clean_and_categorize_tags(tags):
    """æ¸…ç†å’Œåˆ†ç±»æ ‡ç­¾"""
    if not tags:
        return 'æœªåˆ†ç±»'
    
    # æ‰©å±•æ ‡ç­¾æ˜ å°„
    tag_mapping = {
        # éŸ³ä¹é£æ ¼
        'top40': 'æµè¡Œé‡‘æ›²', 'hits': 'çƒ­é—¨é‡‘æ›²', 'oldies': 'ç»å…¸è€æ­Œ',
        'rnb': 'èŠ‚å¥è“è°ƒ', 'r&b': 'èŠ‚å¥è“è°ƒ', 'edm': 'ç”µå­èˆæ›²',
        'kpop': 'éŸ©æµ', 'jpop': 'æ—¥æµ', 'cpop': 'åè¯­æµè¡Œ',
        'mandopop': 'åè¯­æµè¡Œ', 'cantopop': 'ç²¤è¯­æµè¡Œ',
        'hiphop': 'å˜»å“ˆ', 'rap': 'è¯´å”±', 'reggae': 'é›·é¬¼',
        'latin': 'æ‹‰ä¸', 'world': 'ä¸–ç•ŒéŸ³ä¹', 'folk': 'æ°‘è°£',
        'blues': 'è“è°ƒ', 'jazz': 'çˆµå£«', 'classical': 'å¤å…¸',
        'rock': 'æ‘‡æ»š', 'metal': 'é‡‘å±', 'pop': 'æµè¡Œ',
        'electronic': 'ç”µå­', 'dance': 'èˆæ›²', 'house': 'æµ©å®¤',
        'techno': 'ç§‘æŠ€', 'trance': 'è¿·å¹»', 'indie': 'ç‹¬ç«‹',
        'country': 'ä¹¡æ‘',
        
        # ç”µå°ç±»å‹
        'fm': 'è°ƒé¢‘', 'am': 'è°ƒå¹…', 'public': 'å…¬å…±å¹¿æ’­',
        'college': 'æ ¡å›­ç”µå°', 'community': 'ç¤¾åŒºç”µå°', 'local': 'æœ¬åœ°',
        'regional': 'åŒºåŸŸ', 'national': 'å…¨å›½', 'international': 'å›½é™…',
        
        # å†…å®¹ç±»å‹
        'news': 'æ–°é—»', 'talk': 'è°ˆè¯', 'sports': 'ä½“è‚²',
        'business': 'è´¢ç»', 'weather': 'å¤©æ°”', 'traffic': 'äº¤é€š',
        'education': 'æ•™è‚²', 'culture': 'æ–‡åŒ–', 'religious': 'å®—æ•™',
        'entertainment': 'å¨±ä¹', 'comedy': 'å–œå‰§', 'lifestyle': 'ç”Ÿæ´»',
        'health': 'å¥åº·', 'fashion': 'æ—¶å°š', 'food': 'ç¾é£Ÿ',
        'travel': 'æ—…æ¸¸', 'children': 'å„¿ç«¥', 'family': 'å®¶åº­'
    }
    
    # åˆ†å‰²æ ‡ç­¾
    tag_list = [tag.strip().lower() for tag in tags.split(',')]
    cleaned_tags = []
    
    for tag in tag_list:
        # ä½¿ç”¨æ˜ å°„æ›¿æ¢
        if tag in tag_mapping:
            if tag_mapping[tag] not in cleaned_tags:
                cleaned_tags.append(tag_mapping[tag])
        # ä¿ç•™æœ‰æ„ä¹‰çš„æ ‡ç­¾
        elif len(tag) > 2 and not tag.isdigit() and tag not in ['the', 'and', 'radio', 'station']:
            if tag not in cleaned_tags:
                cleaned_tags.append(tag)
    
    return ', '.join(cleaned_tags[:3]) if cleaned_tags else 'æœªåˆ†ç±»'

def test_api_endpoint(base_url):
    """æµ‹è¯• API ç«¯ç‚¹æ˜¯å¦å¯ç”¨"""
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    
    try:
        test_url = f"{base_url}/json/stations?limit=1"
        req = urllib.request.Request(test_url, headers={'User-Agent': 'Mozilla/5.0'})
        
        with urllib.request.urlopen(req, context=ssl_context, timeout=10) as response:
            data = response.read().decode('utf-8')
            stations = json.loads(data)
            return len(stations) > 0
    except Exception as e:
        print(f"  âŒ ç«¯ç‚¹æµ‹è¯•å¤±è´¥: {e}")
        return False

def get_total_count(base_url, ssl_context):
    """è·å–ç”µå°æ€»æ•°"""
    try:
        count_url = f"{base_url}/json/stations?limit=1&hidebroken=true"
        req = urllib.request.Request(count_url, headers={'User-Agent': 'Mozilla/5.0'})
        
        with urllib.request.urlopen(req, context=ssl_context, timeout=30) as response:
            if 'x-total-count' in response.headers:
                total_count = int(response.headers['x-total-count'])
                print(f"ğŸ“Š API æŠ¥å‘Šæ€»æ•°: {total_count} ä¸ªç”µå°")
                return total_count
            else:
                test_url = f"{base_url}/json/stations?limit=5000&hidebroken=true"
                req = urllib.request.Request(test_url, headers={'User-Agent': 'Mozilla/5.0'})
                with urllib.request.urlopen(req, context=ssl_context, timeout=30) as resp:
                    data = resp.read().decode('utf-8')
                    stations = json.loads(data)
                    estimated_count = len(stations) * 6
                    print(f"ğŸ“Š ä¼°ç®—æ€»æ•°: {estimated_count} ä¸ªç”µå°")
                    return min(estimated_count, 35000)
    except Exception as e:
        print(f"âŒ è·å–æ€»æ•°å¤±è´¥: {e}")
        return 30000

def fetch_all_stations():
    print("ğŸš€ å¼€å§‹è·å–å…¨çƒç”µå°æ•°æ®...")
    
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    
    potential_urls = [
        "https://de1.api.radio-browser.info",
        "https://at1.api.radio-browser.info", 
        "https://nl1.api.radio-browser.info"
    ]
    
    available_urls = []
    print("ğŸ” æµ‹è¯• API ç«¯ç‚¹å¯ç”¨æ€§...")
    for url in potential_urls:
        print(f"  æµ‹è¯• {url}...")
        if test_api_endpoint(url):
            available_urls.append(url)
            print(f"  âœ… å¯ç”¨")
        else:
            print(f"  âŒ ä¸å¯ç”¨")
    
    if not available_urls:
        print("ğŸ’¥ æ‰€æœ‰ç«¯ç‚¹éƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨ de1 ä½œä¸ºå¤‡ç”¨")
        available_urls = ["https://de1.api.radio-browser.info"]
    
    print(f"ğŸ¯ å¯ç”¨ç«¯ç‚¹: {available_urls}")
    
    all_stations = []
    max_attempts = 3
    
    for base_url in available_urls:
        print(f"\nğŸ“¡ ä½¿ç”¨ç«¯ç‚¹: {base_url}")
        
        try:
            total_count = get_total_count(base_url, ssl_context)
            
            page_size = 1000
            pages = math.ceil(total_count / page_size)
            max_pages = 35
            pages = min(pages, max_pages)
            
            print(f"ğŸ“„ è®¡åˆ’è·å– {pages} é¡µæ•°æ®ï¼Œç›®æ ‡: {total_count} ä¸ªç”µå°...")
            
            successful_pages = 0
            for page in range(pages):
                offset = page * page_size
                url = f"{base_url}/json/stations?offset={offset}&limit={page_size}&hidebroken=true"
                
                for attempt in range(max_attempts):
                    try:
                        print(f"  æ­£åœ¨è·å–ç¬¬ {page + 1}/{pages} é¡µ (offset: {offset})...")
                        
                        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
                        with urllib.request.urlopen(req, context=ssl_context, timeout=60) as response:
                            data = response.read().decode('utf-8')
                            stations = json.loads(data)
                            
                            if stations:
                                all_stations.extend(stations)
                                successful_pages += 1
                                print(f"  âœ… è·å–åˆ° {len(stations)} ä¸ªç”µå°")
                                print(f"  ğŸ“ˆ ç´¯è®¡: {len(all_stations)} ä¸ªç”µå°")
                                break
                            else:
                                print(f"  âš ï¸ ç¬¬ {page + 1} é¡µæ²¡æœ‰æ•°æ®ï¼Œå¯èƒ½å·²åˆ°æœ«å°¾")
                                break
                                
                    except Exception as e:
                        print(f"  âŒ ç¬¬ {page + 1} é¡µè·å–å¤±è´¥ (å°è¯• {attempt + 1}/{max_attempts}): {e}")
                        if attempt < max_attempts - 1:
                            time.sleep(2)
                        else:
                            print(f"  ğŸ’¥ ç¬¬ {page + 1} é¡µè·å–å¤±è´¥ï¼Œè·³è¿‡")
                            break
                
                time.sleep(1)
                
                if page > 2 and len(all_stations) == 0:
                    print("ğŸ’¥ è¿ç»­å¤šé¡µæ²¡æœ‰æ•°æ®ï¼Œæå‰ç»“æŸ")
                    break
                    
            print(f"ğŸ“Š ä» {base_url} æˆåŠŸè·å– {successful_pages}/{pages} é¡µæ•°æ®")
                    
        except Exception as e:
            print(f"âŒ ç«¯ç‚¹ {base_url} å¤„ç†å¤±è´¥: {e}")
            continue
        
        if len(all_stations) >= 28000:
            print("ğŸ¯ å·²è·å–æ¥è¿‘å®Œæ•´æ•°æ®ï¼Œæå‰ç»“æŸ")
            break
    
    return all_stations

def fetch_additional_stations():
    """ä½¿ç”¨ä¸åŒæ’åºæ–¹å¼è·å–æ›´å¤šæ•°æ®"""
    print("\nğŸ”„ ä½¿ç”¨ä¸åŒæ’åºæ–¹å¼è·å–è¡¥å……æ•°æ®...")
    
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    
    base_url = "https://de1.api.radio-browser.info"
    additional_stations = []
    
    sort_methods = [
        "order=votes",
        "order=clickcount", 
        "order=name",
        "order=country",
        "order=state",
        "order=language",
        "order=tags"
    ]
    
    for i, sort_method in enumerate(sort_methods):
        try:
            url = f"{base_url}/json/stations?limit=5000&hidebroken=true&{sort_method}"
            print(f"  è¡¥å……æŸ¥è¯¢ {i + 1}/{len(sort_methods)}: {sort_method}")
            
            req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
            with urllib.request.urlopen(req, context=ssl_context, timeout=60) as response:
                data = response.read().decode('utf-8')
                stations = json.loads(data)
                
                if stations:
                    additional_stations.extend(stations)
                    print(f"  âœ… è·å–åˆ° {len(stations)} ä¸ªç”µå°")
                else:
                    print(f"  âš ï¸ æŸ¥è¯¢æ²¡æœ‰è¿”å›æ•°æ®")
            
            time.sleep(1)
            
        except Exception as e:
            print(f"  âŒ è¡¥å……æŸ¥è¯¢å¤±è´¥: {e}")
            continue
    
    return additional_stations

def process_stations_data(raw_stations):
    """å¤„ç†åŸå§‹ç”µå°æ•°æ®"""
    print("ğŸ”„ å¼€å§‹å¤„ç†æ•°æ®...")
    
    if not raw_stations:
        print("ğŸ’¥ æ²¡æœ‰åŸå§‹æ•°æ®å¯å¤„ç†")
        return []
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(raw_stations)} ä¸ªç”µå°")
    
    # æ•°æ®å»é‡
    unique_stations = []
    seen_uuids = set()
    
    for station in raw_stations:
        uuid = station.get('stationuuid')
        if uuid and uuid not in seen_uuids:
            seen_uuids.add(uuid)
            unique_stations.append(station)
    
    print(f"ğŸ”„ å»é‡åå‰©ä½™ {len(unique_stations)} ä¸ªç”µå°")
    
    # æ•°æ®æ¸…æ´—å’Œä¼˜åŒ–
    processed_stations = []
    valid_count = 0
    invalid_count = 0
    
    for station in unique_stations:
        has_url = station.get('url_resolved') or station.get('url')
        has_name = station.get('name') and station.get('name', '').strip()
        
        if has_url and has_name:
            # ä¼˜åŒ–æ ‡ç­¾
            raw_tags = station.get('tags') or ''
            cleaned_tags = clean_and_categorize_tags(raw_tags)
            
            processed_station = {
                'stationuuid': station.get('stationuuid'),
                'name': station.get('name', '').strip(),
                'country': station.get('country', 'Unknown'),
                'countrycode': station.get('countrycode', ''),
                'url_resolved': station.get('url_resolved') or station.get('url'),
                'tags': cleaned_tags,
                'language': (station.get('language') or '').lower(),
                'votes': station.get('votes', 0),
                'geo_lat': station.get('geo_lat'),
                'geo_long': station.get('geo_long'),
                'lastchecktime': station.get('lastchecktime'),
                'clickcount': station.get('clickcount', 0),
                'bitrate': station.get('bitrate', 0),
                'codec': station.get('codec', '')
            }
            processed_stations.append(processed_station)
            valid_count += 1
        else:
            invalid_count += 1
    
    # æŒ‰æŠ•ç¥¨æ•°æ’åº
    processed_stations.sort(key=lambda x: x.get('votes', 0), reverse=True)
    
    print(f"ğŸ§¹ æ•°æ®æ¸…æ´—å®Œæˆ:")
    print(f"  âœ… æœ‰æ•ˆç”µå°: {valid_count}")
    print(f"  âŒ æ— æ•ˆç”µå°: {invalid_count}")
    print(f"  ğŸ“Š æ€»è®¡: {len(processed_stations)} ä¸ªç”µå°")
    
    return processed_stations

def split_by_region(stations, last_updated):
    """æŒ‰åœ°åŒºåˆ†ç‰‡æ•°æ®"""
    print("ğŸŒ å¼€å§‹æŒ‰åœ°åŒºåˆ†ç‰‡æ•°æ®...")
    
    if not stations:
        print("âš ï¸ æ²¡æœ‰æ•°æ®å¯åˆ†åŒº")
        return
    
    region_countries = {
        'asia': [
            'China', 'Japan', 'South Korea', 'India', 'Indonesia', 'Thailand', 
            'Vietnam', 'Malaysia', 'Philippines', 'Singapore', 'Taiwan', 'Hong Kong',
            'Bangladesh', 'Pakistan', 'Sri Lanka', 'Nepal', 'Bhutan', 'Maldives',
            'Myanmar', 'Cambodia', 'Laos', 'Mongolia', 'North Korea', 'Brunei',
            'Timor-Leste', 'Afghanistan', 'Armenia', 'Azerbaijan', 'Bahrain',
            'Georgia', 'Iran', 'Iraq', 'Israel', 'Jordan', 'Kazakhstan', 'Kuwait',
            'Kyrgyzstan', 'Lebanon', 'Oman', 'Qatar', 'Saudi Arabia', 'Syria',
            'Tajikistan', 'Turkey', 'Turkmenistan', 'United Arab Emirates', 'Uzbekistan', 'Yemen'
        ],
        'europe': [
            'United Kingdom', 'Germany', 'France', 'Italy', 'Spain', 'Netherlands',
            'Sweden', 'Norway', 'Finland', 'Denmark', 'Switzerland', 'Austria',
            'Belgium', 'Ireland', 'Portugal', 'Poland', 'Russia', 'Ukraine',
            'Czech Republic', 'Hungary', 'Romania', 'Greece', 'Bulgaria', 'Serbia',
            'Croatia', 'Slovakia', 'Belarus', 'Lithuania', 'Latvia', 'Estonia',
            'Slovenia', 'Luxembourg', 'Malta', 'Cyprus', 'Iceland', 'Albania',
            'Bosnia', 'Macedonia', 'Montenegro', 'Moldova', 'Monaco', 'San Marino',
            'Vatican', 'Liechtenstein', 'Andorra'
        ],
        'americas': [
            'United States', 'Canada', 'Mexico', 'Brazil', 'Argentina', 'Chile',
            'Colombia', 'Peru', 'Venezuela', 'Cuba', 'Ecuador', 'Dominican Republic',
            'Guatemala', 'Bolivia', 'Haiti', 'Paraguay', 'Uruguay', 'Jamaica',
            'Trinidad', 'Bahamas', 'Panama', 'Costa Rica', 'Puerto Rico', 'Honduras',
            'El Salvador', 'Nicaragua', 'Barbados', 'Saint Lucia', 'Grenada',
            'Suriname', 'Guyana', 'Belize', 'Bahamas', 'Saint Vincent', 'Antigua', 'Barbuda'
        ],
        'africa': [
            'South Africa', 'Egypt', 'Nigeria', 'Kenya', 'Morocco', 'Ethiopia',
            'Ghana', 'Tanzania', 'Algeria', 'Uganda', 'Sudan', 'Angola',
            'Mozambique', 'Madagascar', 'Cameroon', 'Ivory Coast', 'Senegal',
            'Zambia', 'Zimbabwe', 'Tunisia', 'Libya', 'Congo', 'Democratic Republic',
            'Somalia', 'Mali', 'Burkina Faso', 'Malawi', 'Niger', 'Chad',
            'Guinea', 'Rwanda', 'Benin', 'Burundi', 'South Sudan', 'Togo',
            'Sierra Leone', 'Central African', 'Liberia', 'Mauritania', 'Eritrea',
            'Namibia', 'Gambia', 'Botswana', 'Gabon', 'Lesotho', 'Guinea-Bissau',
            'Equatorial Guinea', 'Mauritius', 'Eswatini', 'Djibouti', 'Comoros', 'Cabo Verde'
        ],
        'oceania': [
            'Australia', 'New Zealand', 'Fiji', 'Papua New Guinea', 'New Caledonia',
            'Solomon Islands', 'Vanuatu', 'Samoa', 'Tonga', 'Micronesia',
            'Kiribati', 'Marshall Islands', 'Palau', 'Nauru', 'Tuvalu'
        ]
    }
    
    total_regional_stations = 0
    
    for region, countries in region_countries.items():
        region_stations = []
        for station in stations:
            country = station.get('country', '')
            if country and country != 'Unknown':
                country_lower = country.lower()
                for country_name in countries:
                    if country_name.lower() in country_lower or country_lower in country_name.lower():
                        region_stations.append(station)
                        break
        
        output = {
            'lastUpdated': last_updated,
            'totalStations': len(region_stations),
            'region': region,
            'countries': countries,
            'stations': region_stations
        }
        
        with open(f'data/{region}-stations.json', 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… {region}åœ°åŒº: {len(region_stations)} ä¸ªç”µå°")
        total_regional_stations += len(region_stations)
    
    print(f"ğŸ“ˆ åœ°åŒºåˆ†ç‰‡å®Œæˆï¼æ€»å…± {total_regional_stations} ä¸ªåœ°åŒºç”µå°")

def main():
    """ä¸»å‡½æ•°"""
    try:
        os.makedirs('data', exist_ok=True)
        
        current_time = datetime.now().isoformat()
        
        print("=" * 60)
        print("ğŸ¯ å…¨çƒå¹¿æ’­ç”µå°æ•°æ®é‡‡é›† - ä¼˜åŒ–ç‰ˆæœ¬")
        print("=" * 60)
        
        raw_stations = fetch_all_stations()
        
        if len(raw_stations) < 25000:
            print(f"\nğŸ”„ ç¬¬ä¸€é˜¶æ®µåªè·å–äº† {len(raw_stations)} ä¸ªç”µå°ï¼Œå¼€å§‹ç¬¬äºŒé˜¶æ®µ...")
            additional_stations = fetch_additional_stations()
            raw_stations.extend(additional_stations)
            
            unique_raw = []
            seen = set()
            for station in raw_stations:
                uuid = station.get('stationuuid')
                if uuid and uuid not in seen:
                    seen.add(uuid)
                    unique_raw.append(station)
            raw_stations = unique_raw
            print(f"ğŸ“Š åˆå¹¶ååŸå§‹æ•°æ®: {len(raw_stations)} ä¸ªç”µå°")
        
        processed_stations = process_stations_data(raw_stations)
        
        if not processed_stations:
            print("ğŸ’¥ æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            processed_stations = []
        
        curated_output = {
            'lastUpdated': current_time,
            'totalStations': len(processed_stations),
            'source': 'Radio Browser API',
            'note': f'é€šè¿‡åˆ†é¡µå’Œå¤šç§æ’åºæ–¹å¼é‡‡é›†ï¼ŒåŸå§‹æ•°æ®: {len(raw_stations)} ä¸ªç”µå°',
            'stations': processed_stations
        }
        
        with open('data/curated-stations.json', 'w', encoding='utf-8') as f:
            json.dump(curated_output, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç²¾é€‰æ•°æ®ä¿å­˜å®Œæˆï¼")
        print(f"  æ–‡ä»¶: data/curated-stations.json")
        print(f"  æœ‰æ•ˆç”µå°æ•°: {len(processed_stations)}")
        print(f"  åŸå§‹ç”µå°æ•°: {len(raw_stations)}")
        
        if processed_stations:
            split_by_region(processed_stations, current_time)
        else:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯åˆ†åŒº")
        
        print("\nğŸ‰ æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
        
    except Exception as e:
        print(f"ğŸ’¥ é¢„å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
